<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>Robotic Control via Embodied Chain-of-Thought Reasoning</title>
  <meta name="description" content="Robotic Control via Embodied Chain-of-Thought Reasoning">
  <meta name="keywords" content="ECoT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:title" content="Robotic Control via Embodied Chain-of-Thought Reasoning">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="Robotic Control via Embodied Chain-of-Thought Reasoning">
  <meta property="og:image" content="https://embodied-cot.github.io/images/ecot_teaser/ecot_teaser-1.png" />
  <!-- todo -->
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="1082" />
  <meta property="og:image:height" content="639" />
  <meta property="og:url" content="https://embodied-cot.github.io" />
  <meta property="og:description" content="Project page for Embodied Chain-of-Thought Reasoning" />
  <meta name="twitter:title" content="Robotic Control via Embodied Chain-of-Thought Reasoning" />
  <meta name="twitter:description" content="Project page for Embodied Chain-of-Thought Reasoning" />
  <meta name="twitter:image" content="https://embodied-cot.github.io/images/ecot_teaser/ecot_teaser-1.png" />

  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-PFJ2DFW');</script>
  <!-- End Google Tag Manager -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/vlmaps_icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PFJ2DFW" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Robotic Control via Embodied Chain-of-Thought Reasoning</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="">Michał Zawalski</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://verityw.github.io/">William Chen</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Karl Pertsch</a><sup>1, 2</sup>,</span>
              <span class="author-block">
                <a href="https://www.oiermees.com/">Oier Mees</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="">Chelsea Finn</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><sup>1</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>UC Berkeley,</span>
              <span class="author-block"><sup>2</sup>Stanford University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block"> <!-- TODO -->
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block"> <!-- TODO -->
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark"> <!-- TODO -->
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video id="teaser" autoplay controls muted loop playsinline height="100%">
          <source src="static/videos/SummaryVid.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Embodied Chain-of-Thought Reasoning enables robots to textually reason about their task and actions.
        </h2>
      </div>
    </div>
  </section>

  <!--
  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve has-text-centered">
            <video poster="" id="steve video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/back_and_forth_x4_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move back and forth between the box and the keyboard</p>

          </div>
          <div class="item item-chair-tp has-text-centered">
            <video poster="" id="chair-tp video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/right_between_v3_x4_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move right 1.5 meters, then move left 3 meters, then move left 1.5 meters</p>
          </div>
          <div class="item item-chair-tp has-text-centered">
            <video poster="" id="chair-tp video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/right_left_right_x4_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move right 1.5 meters, then move left 3 meters, then move left 1.5 meters</p>
          </div>
          <div class="item item-shiba has-text-centered">
            <video poster="" id="shiba video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/move_to_plant_x8_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move to the plant</p>
          </div>
          <div class="item item-fullbody has-text-centered">
            <video poster="" id="fullbody video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/move_in_between_x4_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move in between the wooden box and the chair</p>
          </div>
        </div>
      </div>
    </div>
  </section>
  -->

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              A key limitation of learned robot control policies is their inability to generalize outside their training
              data. Recent works on vision-language-action models (VLAs) have shown that the use of large, internet
              pre-trained vision-language models as the backbone of learned robot policies can substantially improve
              their robustness and generalization ability. Yet, one of the most exciting capabilities of large
              vision-language models in other domains is their ability to reason iteratively through complex problems.
              Can that same capability be brought into robotics to allow policies to improve performance by reasoning
              about a given task before acting? Naive use of "chain-of-thought" (CoT) style prompting is significantly
              less effective with standard VLAs because of the relatively simple training examples that are available to
              them. Additionally, purely semantic reasoning about sub-tasks, as is common in regular CoT, is
              insufficient for robot policies that need to ground their reasoning in sensory observations and the robot
              state. To this end, we introduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which we train
              VLAs to perform multiple steps of reasoning about plans, sub-tasks, motions, and visually grounded
              features like object bounding boxes and end effector positions, before predicting the robot action. We
              design a scalable pipeline for generating synthetic training data for ECoT on large robot datasets. We
              demonstrate, that ECoT increases the absolute success rate of OpenVLA, the current strongest open-source
              VLA policy, by 28% across challenging generalization tasks, without any additional robot training data.
              Additionally, ECoT makes it easier for humans to interpret a policy's failures and correct its behavior
              using natural language.
            </p>
          </div>
        </div>
      </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Approach</h2>
          <div class="content has-text-justified has-text-centered">
            <h3 class="title is-4">Embodied Chain-of-Thought Reasoning (ECoT)</h3>

            <img src="static/images/ecot_schematic/ecot_schematic-1.png" />
            <p style="text-align: center;"><small>Schematic illustrating the reasoning steps produced by our embodied
                chain-of-thought policy. Pink boxes represent embodied reasoning steps.</small></p>

            <p>
              We train a vision-language-action policy (VLA) to autoregressively generate textual reasoning (the
              "embodied chain of thought", or ECoT) in response to commands and observations before it chooses a robot
              action. Such reasoning contains high-level reasoning steps (task, plan, and sub-task) to encourage the
              model to "think carefully" and low-level features (movement, gripper position, and labeled object bounding
              boxes) to get the model to "look carefully."
            </p>
            <p>
              Our ECoT policy is built atop <a href="https://openvla.github.io/">OpenVLA</a>, which fine-tunes a <a
                href="https://github.com/TRI-ML/prismatic-vlms">Prismatic VLMs</a> to take in images
              and instructions to output actions.
            </p>
            <h3 class="title is-4">Data Generation Pipeline</h3>
            <img src="static/images/ecot_data_gen/ecot_data_gen-1.png" />
            <p style="text-align: center;"><small>We use various foundation models as submodules for our embodied
                chain-of-thought data generation pipeline.</small></p>
            <p>
              To train our policies to perform embodied chain-of-thought reasoning, we create a synthetic data
              generation pipeline that leverages numerous foundation models to extract features from robot
              demonstrations to put into corresponding textual reasoning chains. Specifically, we make use of <a
                href="https://github.com/TRI-ML/prismatic-vlms">Prismatic VLMs</a> to caption demonstration scenes; <a
                href="https://github.com/IDEA-Research/GroundingDINO">Grounding DINO</a> to produce bounding boxes
              labeled with entities mentioned in said caption; a hard-coded Python function to convert proprioception to
              textual motion primitives; <a href="https://github.com/inuwamobarak/OWLv2">OWLv2</a> and <a
                href="https://segment-anything.com/">SAM</a> to get the robot gripper position; and <a
                href="https://gemini.google.com/"> Gemini</a> to convert the task instruction, scene description, and
              motion primitives into dense labels for the plan, subtasks, and motion reasonings.
            </p>
            <p>
              We use this pipeline on
              the <a href="https://github.com/rail-berkeley/bridge_data_v2">Bridge V2</a> robot demonstration dataset to
              produce the embodied chain-of-thought data for training our policy.
            </p>
          </div>
        </div>
      </div>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <h2 class="title is-3">Experiments</h2>

                <!-- Interpolating. -->

                <h3 class="title is-4">Primary Comparisons</h3>
                <div class="content has-text-justified">
                  <div class="columns is-vcentered interpolation-panel">
                    <div class="column  has-text-centered">
                      <video autoplay controls muted loop playsinline height="100%">
                        <source src="static/videos/Example_Mushroom_To_Tape.mp4" type="video/mp4">
                      </video>
                    </div>
                  </div>
                  <p>
                    We construct a suite of evaluation tasks to test how well ECoT deals with novel objects,
                    instructions, and spatial relations. We
                    compare our policy against several baselines: <a href="https://openvla.github.io/">OpenVLA</a>, <a
                      href="https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/">RT-2-X</a>,
                    and a naive chain-of-thought VLA
                    that only reasons about high-level plans and sub-tasks, without any embodied reasoning steps.
                  </p>
                  <p>
                    We find that our ECoT policy outperforms all baseline approaches. TODO
                  </p>
                  <img src="static/images/ecot_quali_examples/ecot_quali_examples-1.png" />
                  <p style="text-align: center;"><small>Some example ECoT generations. Left and middle are
                      from successful trajectories, while right is from a failed one.</small></p>
                  We also find that the generated reasonings can give qualitative interpretability
                  insights into why the policy succeeds or fails at a given task. In the left-side example above, the
                  high-level reasoning and
                  low-level detections are all correct, leading to a successful trajectory. In the middle example, the
                  model correctly rephrases the task ("put the leftmost object on the middle object") to be more
                  specific ("put the pink stuffed animal on the blue towel"); this is an emergent behavior, as we do not
                  typically train on rephrasings of this type. Finally, in the right-side example, we see that the
                  policy
                  incorrectly identifies the hammer as a screwdriver, which explains why the robot wrongly reached
                  for it.
                </div>

                <h3 class="title is-4">Human Interventions via Natural Language Corrections</h3>
                <div class="content has-text-justified">
                  <div class="columns is-vcentered interpolation-panel">
                    <div class="column  has-text-centered">
                      <video autoplay controls muted loop playsinline height="100%">
                        <source src="static/videos/CorrectionVid.mp4" type="video/mp4">
                      </video>
                    </div>
                  </div>
                  <p>
                    Embodied chain-of-thought reasoning open a new approach for human-robot interaction: human operators
                    can interpret
                    and debug the robot's "thought process" by issuing natural language hints
                    or corrections, which a large language model (like <a
                      href="https://openai.com/chatgpt/">ChatGPT</a>) can format into an ECoT reasoning text. The ECoT
                    policy can then condition on this fixed reasoning to pick actions.
                  </p>
                  <p>
                    Note that purely instruction-conditioned policies can only have their instruction prompts modified,
                    meaning ECoT is much less restrictive in the kinds of natural language intervention that it accepts.
                  </p>
                  <img src="static/images/ecot_human_intervention/ecot_human_intervention-1.png" />
                  <p style="text-align: center;"><small>Embodied chain-of-thought reasoning enables policies to accept
                      natural language corrections and hints.</small></p>
                  <p>
                    Using such interventions, our policy is able to solve tasks it previously failed on. This
                    improvement boosts our policy's performance on difficult tasks beyond that of OpenVLA and RT-2-X,
                    both with or without modified instruction prompts.
                  </p>
                </div>

                <h3 class="title is-4">Inference Speed Optimizations</h3>
                <div class="content has-text-justified">
                  <p>
                    TODO
                  </p>
                </div>

                <h3 class="title is-4">Code</h3>
                <div class="content has-text-justified">
                  <p>
                    TODO
                  </p>
                  <pre><code>
Example Code
                  </code></pre>
                  <p>
                    TODO
                  </p>
                </div>




              </div>
            </div>


          </div>
      </section>


      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code> @article{Zawalski24-ecot,
            title={Robotic Control via Embodied Chain-of-Thought Reasoning},
            author={Zawalski, Michał and Chen, William and Pertsch, Karl and Mees, Oier and Finn, Chelsea and Levine, Sergey},
            year={2024}
          } </code></pre>
        </div>
      </section>





      <footer class="footer">
        <div class="container">
          <div class="content has-text-centered">
            <a class="icon-link" href="https://arxiv.org/pdf/2210.05714.pdf">
              <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="" class="external-link" disabled>
              <i class="fab fa-github"></i>
            </a>
          </div>
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <p> Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> under a <a
                    href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
                    International</a>
                </p>

              </div>
            </div>
          </div>
        </div>
      </footer>

</body>

</html>